{"cells":[{"cell_type":"markdown","metadata":{"id":"WSUwg_CK1x77"},"source":["# HW 1: Building a Small GPT Model"]},{"cell_type":"markdown","metadata":{"id":"trSKoTHo1x8B"},"source":["Welcome to the first (and last) wet assignment!\n","\n","In this notebook, we will be building a small version of the GPT model.\n","\n","- This notebook is divided into several sections, each with a specific task.\n","- You will find within each cell the place where you are expected to write your code. **Do not change any code outside of this area.**\n","- Each exercise will be in an already defined class or function, with documentation of what it does.\n","- Make sure to read the class or function documentation carefully.\n","\n","\n","For example:\n","\n","        Some Code\n","        # ======= Your Code Starts Here ========\n","                        |\n","                        |\n","                Write your code here\n","                        |\n","                        |\n","        # ======= Your Code Ends Here ========\n","        Some Code\n","\n","- We recommend using Google Colab with a free GPU to enhance the training speed, but it is not necessary.\n","- Remember, the goal of this assignment is not just to get the correct output, but to understand the process of building and training a GPT model. Don't hesitate to ask questions if you're unsure about anything.\n","- We are here to help. If you have any questions, please post them on the relevant section in Piazza: https://piazza.com/technion.ac.il/winter2024/236004\n","\n","Good luck!"]},{"cell_type":"markdown","metadata":{"id":"kfQ1DBCtXBzk"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bHQ_RznC1x8D"},"outputs":[],"source":["        # ======= Your Code Starts Here ========\n","# !pip install datasets\n","        # ======= Your Code Ends Here ========\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BtNzh4kboe9I"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from datasets import Dataset, load_dataset\n","from transformers import GPT2Tokenizer, get_linear_schedule_with_warmup\n","from tqdm import tqdm\n","from typing import Tuple, Optional\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"hKqsVsJQw4eO"},"source":["# Part 1 Attention (30 points)"]},{"cell_type":"markdown","metadata":{"id":"UtlI7O8t1x8G"},"source":["## Part 1.1 Attention Head (20 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"if1kDv9z1x8H"},"outputs":[],"source":["class AttentionHead(nn.Module):\n","    \"\"\"\n","    This class represents an attention head for transformer models.\n","    \"\"\"\n","\n","    def __init__(self, d_input: int, n_hidden: int):\n","        \"\"\"\n","        Initializes the AttentionHead.\n","\n","        Args:\n","            d_input: the dimension of the input\n","            n_hidden: the dimension of the keys, queries, and values\n","        \"\"\"\n","        super().__init__()\n","        self.W_K = nn.Linear(d_input, n_hidden)\n","        self.W_Q = nn.Linear(d_input, n_hidden)\n","        self.W_V = nn.Linear(d_input, n_hidden)\n","        self.n_hidden = n_hidden\n","\n","    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Computes the forward pass of the attention head.\n","\n","        Args:\n","            x (torch.Tensor): The input tensor. Shape: (batch_size, seq_length, d_input)\n","            attn_mask (Optional[torch.Tensor]): The causal mask tensor. If provided, it acts as an attention mask\n","            that determines which tokens in the sequence should be attended to. It's a 3D tensor where the value at\n","            position [b, i, j] is 1 if the token at position i in batch b should attend to the token at position j,\n","            and 0 otherwise. If not provided (None), ignore it.\n","            Shape: (batch_size, seq_length, seq_length)\n","\n","        Returns:\n","            attn_output (torch.Tensor): The output tensor after attention. Shape: (batch_size, seq_length, n_hidden)\n","            attn_score (torch.Tensor): The attention score tensor. Shape: (batch_size, seq_length, seq_length)\n","        \"\"\"\n","        attn_output, attn_score = None, None\n","        # ======= Your Code Starts Here ========\n","\n","        # ======= Your Code Ends Here ========\n","\n","        return attn_output, attn_score"]},{"cell_type":"markdown","metadata":{"id":"kj4U_FdxYn3N"},"source":["## Part 1.2 Multihead Attention (10 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsmMS8Hh1x8I"},"outputs":[],"source":["class MultiheadAttention(nn.Module):\n","    def __init__(self, d_input: int, n_hidden: int, num_heads: int):\n","        \"\"\"\n","        Initializes the MultiheadAttention.\n","\n","        Args:\n","            d_input (int): The dimension of the input.\n","            n_hidden: the hidden dimenstion for the attention layer\n","            num_heads (int): The number of attention heads.\n","        Attributes:\n","            attention_heads (nn.ModuleList): A list of attention heads.\n","            W_proj (nn.Linear): A linear layer for projecting the concatenated outputs of the attention heads back\n","            to the original dimension.\n","        \"\"\"\n","\n","        super().__init__()\n","\n","        # ======= Your Code Starts Here ========\n","\n","        # ======= Your Code Ends Here ========\n","\n","    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Executes the forward pass of the multi-head attention mechanism.\n","\n","        Args:\n","            x (torch.Tensor): The input tensor. It has a shape of (batch_size, seq_length, d_input).\n","            attn_mask (Optional[torch.Tensor]): The attention mask tensor. If provided, it serves as an attention guide\n","            that specifies which tokens in the sequence should be attended to. It's a 3D tensor where the value at\n","            position [b, i, j] is 1 if the token at position i in batch b should attend to the token at position j,\n","            and 0 otherwise. If not provided (None), ignore it.\n","            Shape: (batch_size, seq_length, seq_length)\n","\n","        Returns:\n","            attn_output (torch.Tensor): The output tensor after applying multi-head attention. It has a shape of\n","            (batch_size, seq_length, d_input).\n","\n","        This method computes the multi-head attention by looping through each attention head, collecting the outputs,\n","        concatenating them together along the hidden dimension, and then projecting them back into the output dimension\n","        (d_input). It returns both the final attention outputs as well as the attn_scores from each head.\n","        \"\"\"\n","        attn_output, attn_scores = None, None\n","\n","        # ======= Your Code Starts Here ========\n","\n","        # ======= Your Code Ends Here ========\n","        return attn_output, attn_scores"]},{"cell_type":"markdown","metadata":{"id":"HJRCu-O31x8I"},"source":["# Part 2 GPT (30 points)"]},{"cell_type":"markdown","metadata":{"id":"XTWQcT1Z1x8J"},"source":["## Part 2.1 Tranformer Block (5 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"opucoH1a1x8K"},"outputs":[],"source":["# FFN class is alreay implemented for you\n","class FFN(nn.Module):\n","    \"\"\"\n","    This class represents a Feed-Forward Network (FFN) layer.\n","    \"\"\"\n","    def __init__(self, d_input: int, n_hidden: int):\n","        \"\"\"\n","        Args:\n","            d_input (int): The dimension of the input.\n","            n_hidden (int): The hidden dimension for the FFN.\n","        Attributes:\n","            ffn (nn.Sequential): A sequential container of the FFN layers.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.ffn = nn.Sequential(\n","            nn.Linear(d_input, n_hidden),\n","            nn.GELU(),\n","            nn.Linear(n_hidden, d_input),\n","        )\n","\n","    def forward(self, x: torch.Tensor)-> torch.Tensor:\n","        \"\"\"\n","        Executes the forward pass of the FFN.\n","\n","        Args:\n","            x (torch.Tensor): The input tensor. Shape: (batch_size, seq_length, d_input)\n","\n","        Returns:\n","            out (torch.Tensor): The output tensor of the FFN. Shape: (batch_size, seq_length, d_input)\n","        \"\"\"\n","        return self.ffn(x)\n","\n","\n","class TranformerBlock(nn.Module):\n","    \"\"\"\n","    This class represents a Transformer block.\n","    \"\"\"\n","    def __init__(self, d_input: int, attn_dim: int, mlp_dim: int, num_heads: int):\n","        \"\"\"\n","        Args:\n","            d_input (int): The dimension of the input.\n","            attn_dim (int): The hidden dimension for the attention layer.\n","            mlp_dim (int): The hidden dimension for the FFN.\n","            num_heads (int): The number of attention heads.\n","\n","        Attributes:\n","            layer_norm_1 (nn.LayerNorm): The first layer normalization layer for the attention.\n","            layer_norm_2 (nn.LayerNorm): The second layer normalization layer for the FFN.\n","            attention (MultiheadAttention): The multi-head attention mechanism.\n","            ffn (FFN): The feed-forward network.\n","        \"\"\"\n","        super().__init__()\n","        # ======= Your Code Starts Here ========\n","\n","        # ======= Your Code Ends Here ========\n","\n","    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Executes the forward pass of the Transformer block.\n","\n","        Args:\n","            x (torch.Tensor): The input tensor. Shape: (batch_size, seq_length, d_input)\n","            attn_mask (torch.Tensor): The attention mask tensor. If provided, it serves as an attention guide\n","            that specifies which tokens in the sequence should be attended to. It's a 3D tensor where the value at\n","            position [b, i, j] is 1 if the token at position i in batch b should attend to the token at position j,\n","            and 0 otherwise. If not provided (None), no specific attention pattern is enforced.\n","            Shape: (batch_size, seq_length, seq_length)\n","\n","        Returns:\n","            x (torch.Tensor): The output tensor after passing through the Transformer block. Shape: (batch_size, seq_length, d_input)\n","            attn_scores (torch.Tensor): The attention weights of each of the attention heads. Shape: (batch_size, num_heads, seq_length, seq_length)\n","        \"\"\"\n","        attn_scores = None\n","        # ======= Your Code Starts Here ========\n","\n","        # ======= Your Code Ends Here ========\n","        return x, attn_scores"]},{"cell_type":"markdown","metadata":{"id":"OCT1xysY1x8K"},"source":["## Part 2.2 Transformer (10 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7KsYPoq1x8K"},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(self, d_input: int, attn_dim: int, mlp_dim: int, num_heads: int, num_layers: int):\n","        \"\"\"\n","        Initializes the Transformer.\n","\n","        Args:\n","            d_input (int): The dimension of the input.\n","            attn_dim (int): The hidden dimension of the attention layer.\n","            mlp_dim (int): The hidden dimension of the FFN layer.\n","            num_heads (int): The number of heads in the attention layer.\n","            num_layers (int): The number of attention layers.\n","\n","        Attributes:\n","            transformer_blocks (nn.ModuleList): A list of Transformer blocks.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.transformer_blocks = None\n","        # ======= Your Code Starts Here ========\n","\n","        # ======= Your Code Ends Here ========\n","\n","    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor, return_attn=False)-> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n","        \"\"\"\n","        Executes the forward pass of the GPT model.\n","\n","        Args:\n","            x (torch.Tensor): The input tensor. Shape: (batch_size, seq_length, d_input)\n","            attn_mask (torch.Tensor): The attention mask tensor. Shape: (batch_size, seq_length, seq_length)\n","            return_attn (bool): If True, return the attention weights of each of the attention heads for each of the layers.\n","\n","        Returns:\n","            output (torch.Tensor): The output tensor after passing through the GPT model. Shape: (batch_size, seq_length, d_input)\n","            collected_attns (Optional[torch.Tensor]): If return_attn is False, return None. Otherwise return the attention weights\n","            of each of each of the attention heads for each of the layers. Shape: (batch_size, num_layers, num_heads, seq_length, seq_length)\n","        \"\"\"\n","        output, collected_attns = None, None\n","        # ======= Your Code Starts Here ========\n","\n","        # ======= Your Code Ends Here ========\n","        return output, collected_attns"]},{"cell_type":"markdown","metadata":{"id":"jhwzHmwL1x8L"},"source":["## Run Transformer Tests (no implementation needed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AvJQutox1x8L"},"outputs":[],"source":["def run_transformer_tests():\n","    # Define parameters\n","    n_ctx = 16\n","    batch_size = 4\n","    d_input = 32\n","    d_embed = 64\n","    n_layers = 4\n","    n_heads = 4\n","    attn_dim = d_embed // n_heads\n","    mlp_dim = d_input * 4\n","\n","    # Initialize the model\n","    model = Transformer(d_input=d_input, attn_dim=attn_dim, mlp_dim=mlp_dim, num_heads=n_heads, num_layers=n_layers).to(device)\n","\n","    # Create a random input tensor\n","    input_tensor = torch.randn(batch_size, n_ctx, d_input).to(device)\n","\n","    # Test Case 1: Regular forward pass\n","    print(\"Test Case 1: Regular forward pass\")\n","    with torch.no_grad():\n","        output_tensor, attention_weights = model(input_tensor, attn_mask=None)\n","        assert attention_weights is None\n","        assert output_tensor.shape == (batch_size, n_ctx, d_input), f\"Unexpected output shape {output_tensor.shape}\"\n","    print(\"Test Case 1 Passed!\")\n","\n","    # Test Case 2: Collect attentions\n","    print(\"Test Case 2: Collect attentions\")\n","    with torch.no_grad():\n","        output_tensor, attention_weights = model(input_tensor, attn_mask=None, return_attn=True)\n","        assert output_tensor.shape == (batch_size, n_ctx, d_input), f\"Unexpected output shape {output_tensor.shape}\"\n","        assert attention_weights.shape == (batch_size, n_heads, n_heads, n_ctx, n_ctx), f\"Unexpected attention weights shape {attention_weights.shape}\"\n","    print(\"Test Case 2 Passed!\")\n","\n","    # Test Case 3: With attention mask\n","    print(\"Test Case 3: With attention mask\")\n","    attention_mask = torch.zeros(batch_size, n_ctx, n_ctx).to(device)\n","    attention_mask[:, torch.arange(n_ctx), torch.arange(n_ctx)] = 1\n","    attention_mask[:, torch.arange(n_ctx)[1:], torch.arange(n_ctx)[:-1]] = 1\n","\n","    with torch.no_grad():\n","        output_tensor, attention_weights = model(input_tensor, attn_mask=attention_mask, return_attn=True)\n","        assert torch.all(attention_weights.permute(1, 2, 0, 3, 4)[:, :, attention_mask == 0] == 0).item()\n","    print(\"Test Case 3 Passed!\")\n","\n","# Run the test cases\n","run_transformer_tests()"]},{"cell_type":"markdown","metadata":{"id":"xAq8kKHF1x8L"},"source":["## Configs (no implementation needed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IwHVJgnN1x8L"},"outputs":[],"source":["class ModelConfig:\n","    def __init__(self, d_embed: int = 192, debug: bool = False, layer_norm_eps: float = 1e-5,\n","                 d_vocab: int = 50257 + 1, n_ctx: int = 32, n_heads: int = 4, n_layers: int = 4):\n","        self.d_embed = d_embed\n","        self.debug = debug\n","        self.layer_norm_eps = layer_norm_eps\n","        self.d_vocab = d_vocab\n","        self.n_ctx = n_ctx\n","        self.n_heads = n_heads\n","        self.d_head = self.d_embed // self.n_heads\n","        self.n_layers = n_layers\n","\n","    def __repr__(self):\n","        return '\\n'.join([f'{k}: {v}' for k, v in self.__dict__.items()])\n","\n","class TrainConfig:\n","    def __init__(self, batch: int = 4, epochs: int = 10, lr: float = 5e-4, warmup_steps_ratio: float = 0.1,\n","                 device: torch.device = device):\n","        self.batch = batch\n","        self.epochs = epochs\n","        self.lr = lr\n","        self.warmup_steps_ratio = warmup_steps_ratio\n","        self.device = device\n","\n","    def __repr__(self):\n","        return '\\n'.join([f'{k}: {v}' for k, v in self.__dict__.items()])"]},{"cell_type":"markdown","metadata":{"id":"ZOZVSJjC1x8L"},"source":["## Part 2.3 Embedding (5 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YOlkyu-P1x8L"},"outputs":[],"source":["class Embed(nn.Module):\n","    def __init__(self, config: ModelConfig):\n","        \"\"\"\n","        Args:\n","            config (ModelConfig): The model configuration.\n","        \"\"\"\n","        super().__init__()\n","        self.d_embed = config.d_embed\n","        self.d_vocab = config.d_vocab\n","        self.embed = nn.Embedding(self.d_vocab, self.d_embed, device=device)\n","    def forward(self, input_ids):\n","        return self.embed(input_ids)\n","\n","# Implement learnt positional encoding\n","class PosEmbed(nn.Module):\n","    def __init__(self, config: ModelConfig):\n","        \"\"\"\n","        Args:\n","            config (ModelConfig): The model configuration.\n","        \"\"\"\n","        super().__init__()\n","        self.d_embed = config.d_embed\n","        self.n_ctx = config.n_ctx\n","        self.pos_embed = nn.Embedding(self.n_ctx, self.d_embed, device=device)\n","\n","    def forward(self, input_ids):\n","        indices = None\n","        # ======= Your Code Starts Here ========\n","\n","        # ======= Your Code Ends Here ========\n","        return self.pos_embed(indices)"]},{"cell_type":"markdown","metadata":{"id":"RRpg8OT91x8M"},"source":["## Part 2.4 GPT (10 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1r5io611x8M"},"outputs":[],"source":["class GPT(nn.Module):\n","    def __init__(self, config: ModelConfig, Embed: nn.Module, PosEmbed: nn.Module):\n","        \"\"\"\n","        Initializes the GPT model.\n","\n","        Args:\n","            config (ModelConfig): The model configuration.\n","            Embed (nn.Module): The embedding layer.\n","            PosEmbed (nn.Module): The positional embedding layer.\n","        \"\"\"\n","        super().__init__()\n","        self.d_vocab = config.d_vocab\n","        self.n_ctx = config.n_ctx\n","        self.d_embed = config.d_embed\n","        self.attn_dim = config.d_head\n","        self.mlp_dim = config.d_embed * 4\n","        self.num_heads = config.n_heads\n","        self.num_layers = config.n_layers\n","\n","        self.token_embedding = Embed(config)\n","        self.positional_embedding = PosEmbed(config)\n","\n","        self.transformer = Transformer(\n","            d_input=self.d_embed, attn_dim=self.attn_dim, mlp_dim=self.mlp_dim, num_heads=self.num_heads, num_layers=self.num_layers)\n","\n","        # ======= Your Code Starts Here ========\n","\n","        # ======= Your Code Ends Here ========\n","\n","    def forward(self, input_ids: torch.Tensor, return_attn=False) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n","        \"\"\"\n","        Executes the forward pass of the GPT model.\n","\n","        Args:\n","            input_ids (torch.Tensor): A batch of input ids (right padded). Shape: (batch_size x seq_length).\n","            return_attn (bool): If True, return the attention weights of each of the attention heads for each of the layers.\n","\n","        Returns:\n","            out (torch.Tensor): The logit vector. Shape: (batch_size x seq_length x vocab_size).\n","            attn_scores (Optional[torch.Tensor]): If return_attn is False, return None. Otherwise return the attention weights\n","            of each of each of the attention heads for each of the layers. Shape: (batch_size, num_layers, num_heads, seq_length, seq_length).\n","        \"\"\"\n","        out, attn_scores = None, None\n","        # ======= Your Code Starts Here ========\n","\n","        # ======= Your Code Ends Here ========\n","        return out, attn_scores"]},{"cell_type":"markdown","metadata":{"id":"azmIOsqo1x8M"},"source":["## Generating (no implementation needed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvnVev_M1x8M"},"outputs":[],"source":["model_config = ModelConfig()\n","train_config = TrainConfig()\n","\n","# Load the tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2', max_length=model_config.n_ctx)\n","tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","tokenizer.pad_token = '[PAD]'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEvqx-N51x8M"},"outputs":[],"source":["def generate(model, tokenizer, prompt, max_len=10):\n","    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n","    generated_tokens = input_ids.to(device)\n","\n","    for _ in range(max_len):\n","        with torch.no_grad():\n","            outputs, _ = model(generated_tokens)\n","        next_token = torch.argmax(outputs[:, -1, :], dim=-1).unsqueeze(-1)\n","        generated_tokens = torch.cat((generated_tokens, next_token), dim=-1)\n","\n","    generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=False)\n","    return generated_text"]},{"cell_type":"markdown","metadata":{"id":"Qi6_L8571x8M"},"source":["# Trainning (no implementation needed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mCb4QR71x8M"},"outputs":[],"source":["def train(model: nn.Module, dataloader, optimizer, scheduler, criterion, train_config: TrainConfig):\n","    for epoch in range(train_config.epochs):\n","        epoch_loss = 0\n","        with tqdm(dataloader, unit=\"batch\") as tepoch:\n","            for batch in tepoch:\n","                input_ids = batch['input_ids'].to(device)\n","                optimizer.zero_grad()\n","\n","                # Forward pass\n","                outputs, _ = model(input_ids[:, :-1])\n","                loss = criterion(outputs.view(-1, outputs.size(-1)), input_ids[:, 1:].reshape(-1))\n","                # Backward pass\n","                loss.backward()\n","                optimizer.step()\n","                scheduler.step()\n","\n","                epoch_loss += loss.item()\n","                tepoch.set_postfix(loss=loss.item())\n","\n","        epoch_loss /= len(dataloader)\n","        print(f\"Epoch: {epoch}, Loss: {epoch_loss:0.4f}, LR: {scheduler.get_last_lr()[0]:0.4e}\")"]},{"cell_type":"markdown","metadata":{"id":"58kOcjWK1x8N"},"source":["# Testing (no implementation needed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YEYpJupU1x8N"},"outputs":[],"source":["def test(model: nn.Module, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, unit=\"batch\"):\n","            input_ids = batch['input_ids'].to(device)\n","            outputs, _ = model(input_ids[:, :-1])\n","            loss = criterion(outputs.view(-1, outputs.size(-1)), input_ids[:, 1:].reshape(-1))\n","            total_loss += loss.item()\n","    total_loss /= len(dataloader)\n","    print(f\"Loss: {total_loss:0.4f}\")\n","    if total_loss < 5.5:\n","        print(\"Test Passed!\")\n","    else:\n","        print(\"Test Failed!, Loss is too high\")"]},{"cell_type":"markdown","metadata":{"id":"DLg38XQT1x8N"},"source":["## Model training sainity check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pYopaDC51x8N"},"outputs":[],"source":["# Run me for sanity check of the model\n","\n","# Note that this is not a comprehensive test of the model, and if it pass this test, it does not mean your model is correct.\n","# Even if it does memorize the training set, it does not necessarily mean the model is implemented correctly.\n","# You should check the loss and generated text to see if it makes sense and preferably create your own tests.\n","\n","def sanity_check():\n","    \"\"\"\n","    Runs a sanity check to ensure the model can overfit on a small dataset.\n","    \"\"\"\n","    # Configure the model and training\n","    model_config = ModelConfig()\n","    train_config = TrainConfig(batch=1, epochs=20, lr=5e-4)\n","\n","    # Prepare the data\n","    sentence = \"This course is so much fun! I wish it was longer.\"\n","    inputs = tokenizer(sentence, return_tensors='pt')\n","    dataset = Dataset.from_dict({'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask']})\n","    dataset = dataset.with_format(\"torch\", device=train_config.device)\n","    data_loader = DataLoader(dataset, batch_size=train_config.batch, shuffle=False)\n","\n","    model = GPT(model_config, Embed, PosEmbed).to(device)\n","\n","    # Set up the optimizer, loss function, and scheduler\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=train_config.lr, weight_decay=0)\n","    steps = int((train_config.epochs * len(data_loader)) / train_config.batch)\n","    loss_fn = nn.CrossEntropyLoss()\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=steps)\n","\n","    # Train the model\n","    train(model, data_loader, optimizer, scheduler, loss_fn, train_config)\n","\n","    # Test the model's ability to memorize a sentence\n","    print(\"\\n\\t ~~~~~~~~ The model should be able to memorize the sentence from the training set ~~~~~~~~\")\n","\n","    generated = generate(model, tokenizer, prompt=\"This course\", max_len=12)\n","    print(f\"\\nOriginal:\\n\\\"{sentence}\\\"\\n\")\n","    print(f\"\\nGenerated:\\n\\\"{generated}\\\"\\n\")\n","sanity_check()"]},{"cell_type":"markdown","metadata":{"id":"NUp6BO5YKTr9"},"source":["## Dataset (no implementation needed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFAso_r4dMjU"},"outputs":[],"source":["wiki_10k = load_dataset(\"NeelNanda/wiki-10k\")\n","data = wiki_10k['train']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqZSMKm1bOVv"},"outputs":[],"source":["def tokenize(example):\n","    return tokenizer(example['text'], max_length=model_config.n_ctx, padding=True, truncation=True, return_tensors='pt')\n","cols_to_remove = list(data.features.keys())\n","dataset = data.map(tokenize, batched=True, remove_columns=cols_to_remove)\n","dataset = dataset.with_format(\"torch\", device=train_config.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3wSZ-BPX1x8O"},"outputs":[],"source":["splitted_dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n","train_dataset, test_dataset = splitted_dataset['train'], splitted_dataset['test']\n","train_dataloader = DataLoader(train_dataset, batch_size=train_config.batch, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=train_config.batch, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"nwmW3RZU1x8O"},"source":["## Model Initialization and Configuration (no implementation needed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URuMzE2V1x8O"},"outputs":[],"source":["model = GPT(model_config, Embed, PosEmbed).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=train_config.lr, weight_decay=0)\n","num_training_steps = int((train_config.epochs * len(train_dataloader)) / train_config.batch)\n","criterion = nn.CrossEntropyLoss()\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=train_config.warmup_steps_ratio * num_training_steps,\n","    num_training_steps=num_training_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4bFzV5P1x8O"},"outputs":[],"source":["train(model, train_dataloader, optimizer, scheduler, criterion, train_config)"]},{"cell_type":"markdown","metadata":{"id":"DtqNANrx1x8V"},"source":["# Part 3.0 - Exploration & Questions (40 points)"]},{"cell_type":"markdown","metadata":{"id":"9GWfWfDr1x8V"},"source":["## Part 3.1 Hyper Paramaters Search (15 points)"]},{"cell_type":"markdown","metadata":{"id":"Of-iG4LD1x8V"},"source":["### Question (5 points)\n","Explore other hyper-parameters (by editing ModelConfig and TrainConfig).\\\n","Try at least 3 different configurations and describe shortly your insights."]},{"cell_type":"markdown","metadata":{"id":"0j9CYStC1x8V"},"source":["### Answer:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBSEYTdx1x8V"},"outputs":[],"source":["# print default config values\n","print(model_config)\n","print()\n","print(train_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJYcBOf_1x8W"},"outputs":[],"source":["# change the configs as you wish to explore how different hyperparameters affect the model training and performance\n","model_config = ModelConfig()\n","train_config = TrainConfig()\n","# ======= Your Code Starts Here ========\n","\n","# ======= Your Code Ends Here ========"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lP3dWaF61x8W"},"outputs":[],"source":["model = GPT(model_config, Embed, PosEmbed).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=train_config.lr, weight_decay=0)\n","num_training_steps = int((train_config.epochs * len(train_dataloader)) / train_config.batch)\n","criterion = nn.CrossEntropyLoss()\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=train_config.warmup_steps_ratio * num_training_steps,\n","    num_training_steps=num_training_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GhOF2Fpr1x8W"},"outputs":[],"source":["train(model, train_dataloader, optimizer, scheduler, criterion, train_config)"]},{"cell_type":"markdown","metadata":{"id":"n0kom4z01x8W"},"source":["### Test (10 points)\n","Ensure that you can find a configuration that passes the test."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Blqlv6OV1x8W"},"outputs":[],"source":["test(model, test_dataloader, criterion)"]},{"cell_type":"markdown","metadata":{"id":"FVZ9v1cj1x8W"},"source":["## Part 3.2: Sin Cos Positional Encoding (10 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LpbMCsfx1x8X"},"outputs":[],"source":["# Implement the SinCosPosEmbed class as described in the paper by Vaswani et al. (2017), \"Attention Is All You Need.\" https://arxiv.org/pdf/1706.03762.pdf\n","class SinCosPosEmbed(nn.Module):\n","    def __init__(self, config: ModelConfig):\n","        \"\"\"\n","        Args:\n","            config (ModelConfig): The model configuration.\n","        \"\"\"\n","        super().__init__()\n","        self.d_embed = config.d_embed\n","        self.n_ctx = config.n_ctx\n","\n","    def forward(self, input_ids):\n","        \"\"\"\n","        Args:\n","            input_ids (torch.Tensor): The input tensor. Shape: (batch_size, seq_length)\n","        \"\"\"\n","        pos_encodings = None\n","        # ======= Your Code Starts Here ========\n","\n","        # ======= Your Code Ends Here ========\n","        return pos_encodings"]},{"cell_type":"markdown","metadata":{"id":"l0vQc8XB1x8X"},"source":["### Test Sin Cos Positional Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N7TXUImT1x8X"},"outputs":[],"source":["def test_positional_encoding():\n","    # test the positional encoding\n","    # the first encoding should be alternating between 0 and 1\n","    pos_embed = SinCosPosEmbed(model_config)\n","    pos = pos_embed(torch.zeros((1, 10), dtype=torch.long, device=device))\n","    assert torch.all(pos[0, 0::2] == 0).item()\n","    assert torch.all(pos[0, 1::2] == 1).item()\n","    print(pos[0])\n","test_positional_encoding()"]},{"cell_type":"markdown","metadata":{"id":"MgHkZ_fO1x8Y"},"source":["## Sin Cos Positional Encodings Questions (15 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tIzFGgfG1x8Y"},"outputs":[],"source":["# Run this to plot the positional encodings values\n","def plot_positional_encodings(pos_encodings):\n","    plt.figure(figsize=(10,10))\n","    plt.pcolormesh(pos_encodings.detach().cpu().numpy(), cmap='viridis')\n","    plt.xlabel('Embedding Dimensions')\n","    plt.ylabel('Sequence Position')\n","    plt.colorbar()\n","    plt.show()\n","\n","# Generate some positional encodings\n","config = ModelConfig(d_embed=64, n_ctx=32)\n","pos_embed = SinCosPosEmbed(config)\n","pos_encodings = pos_embed(torch.zeros((1, 64)).long().to(device))\n","\n","plot_positional_encodings(pos_encodings)"]},{"cell_type":"markdown","metadata":{"id":"7T2Fj7p81x8Y"},"source":["### Question: (5 points)\n","Describe briefly what is shown in the heatmap (you should verify that the first value is unique and the last value is the same for each positional encoding vector).\\\n","What is the reason for the unique first value (the 0th element in the embedding vector) of each positional encoding in the given heatmap?\\\n","What is the reason for the same last value of each positional encoding?"]},{"cell_type":"markdown","metadata":{"id":"RC6FQEzd1x8Y"},"source":["### Answer:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5o9tv9Pu1x8Z"},"outputs":[],"source":["# Run this to plot the dot product of all positional encodings\n","def plot_scaled_dot_products(pos_encodings):\n","    # Calculate the scaled dot products\n","    dot_products = torch.matmul(pos_encodings, pos_encodings.transpose(0, 1)) / np.sqrt(pos_encodings.shape[1])\n","\n","    # Convert to numpy\n","    dot_products = dot_products.detach().cpu().numpy()\n","\n","    # Plot the heatmap\n","    plt.figure(figsize=(10,10))\n","    sns.heatmap(dot_products, cmap='viridis')\n","    plt.xlabel('Position')\n","    plt.ylabel('Position')\n","    plt.show()\n","\n","# Generate some positional encodings\n","config = ModelConfig(d_embed=512, n_ctx=1024)\n","pos_embed = SinCosPosEmbed(config)\n","pos_encodings = pos_embed(torch.zeros((1, 64)).long().to(device))\n","\n","# Plot the scaled dot products\n","plot_scaled_dot_products(pos_encodings)"]},{"cell_type":"markdown","metadata":{"id":"fU5qD-531x8Z"},"source":["### Question: (10 points)\n","Plot the dot product of all positional encodings, and describe how the model makes use of the displayed information."]},{"cell_type":"markdown","metadata":{"id":"7kGHZ6zx1x8Z"},"source":["### Answer:"]}],"metadata":{"colab":{"collapsed_sections":["hKqsVsJQw4eO"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}